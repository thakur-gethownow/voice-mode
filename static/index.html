<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate" />
  <meta http-equiv="Pragma" content="no-cache" />
  <meta http-equiv="Expires" content="0" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
  <title>AI Voice Assistant</title>
  <style>
    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }
    
    body { 
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      min-height: 100vh;
      padding: 20px;
      color: #333;
    }
    
    .container {
      max-width: 800px;
      margin: 0 auto;
      background: rgba(255, 255, 255, 0.95);
      border-radius: 20px;
      padding: 30px;
      backdrop-filter: blur(10px);
      box-shadow: 0 20px 40px rgba(0, 0, 0, 0.1);
    }
    
    .header {
      text-align: center;
      margin-bottom: 30px;
    }
    
    .header h1 {
      font-size: 2.5rem;
      font-weight: 700;
      background: linear-gradient(135deg, #667eea, #764ba2);
      -webkit-background-clip: text;
      background-clip: text;
      -webkit-text-fill-color: transparent;
      margin-bottom: 10px;
    }
    
    .header p {
      color: #666;
      font-size: 1.1rem;
      font-weight: 300;
    }
    
    .controls {
      display: flex;
      justify-content: center;
      gap: 15px;
      margin-bottom: 25px;
      flex-wrap: wrap;
    }
    
    button { 
      padding: 12px 24px;
      border: none;
      border-radius: 12px;
      font-size: 16px;
      font-weight: 600;
      cursor: pointer;
      transition: all 0.3s ease;
      text-transform: uppercase;
      letter-spacing: 0.5px;
    }
    
    #startBtn {
      background: linear-gradient(135deg, #4CAF50, #45a049);
      color: white;
      box-shadow: 0 4px 15px rgba(76, 175, 80, 0.3);
    }
    
    #startBtn:hover {
      transform: translateY(-2px);
      box-shadow: 0 6px 20px rgba(76, 175, 80, 0.4);
    }
    
    #startBtn:disabled {
      background: #ccc;
      cursor: not-allowed;
      transform: none;
      box-shadow: none;
    }
    
    #stopBtn {
      background: linear-gradient(135deg, #f44336, #d32f2f);
      color: white;
      box-shadow: 0 4px 15px rgba(244, 67, 54, 0.3);
    }
    
    #stopBtn:hover:not(:disabled) {
      transform: translateY(-2px);
      box-shadow: 0 6px 20px rgba(244, 67, 54, 0.4);
    }
    
    #stopBtn:disabled {
      background: #ccc;
      cursor: not-allowed;
      transform: none;
      box-shadow: none;
    }
    
    #testBtn {
      background: linear-gradient(135deg, #2196F3, #1976D2);
      color: white;
      box-shadow: 0 4px 15px rgba(33, 150, 243, 0.3);
    }
    
    #testBtn:hover {
      transform: translateY(-2px);
      box-shadow: 0 6px 20px rgba(33, 150, 243, 0.4);
    }
    
    .model-selector {
      display: flex;
      align-items: center;
      justify-content: center;
      gap: 15px;
      margin-bottom: 25px;
      flex-wrap: wrap;
    }
    
    .model-selector label {
      font-weight: 600;
      color: #555;
      display: flex;
      align-items: center;
      gap: 8px;
    }
    
    #modelSelect {
      padding: 10px 15px;
      border: 2px solid #e0e0e0;
      border-radius: 8px;
      font-size: 14px;
      font-weight: 500;
      background: white;
      color: #333;
      cursor: pointer;
      transition: all 0.3s ease;
      min-width: 280px;
    }
    
    #modelSelect:hover {
      border-color: #667eea;
      box-shadow: 0 2px 10px rgba(102, 126, 234, 0.2);
    }
    
    #modelSelect:focus {
      outline: none;
      border-color: #667eea;
      box-shadow: 0 0 0 3px rgba(102, 126, 234, 0.1);
    }
    
    /* Benchmark Info Styles */
    .benchmark-info {
      background: linear-gradient(135deg, #f5f5f5, #e8e8e8);
      border: 1px solid #ddd;
      border-radius: 12px;
      padding: 15px;
      margin: 15px 0;
      font-size: 12px;
    }
    
    .benchmark-stats {
      display: grid;
      grid-template-columns: 1fr 1fr;
      gap: 8px;
      margin-bottom: 10px;
    }
    
    .benchmark-stats span {
      padding: 4px 8px;
      background: rgba(255, 255, 255, 0.7);
      border-radius: 6px;
      font-weight: 500;
      text-align: center;
    }
    
    .benchmark-controls {
      display: flex;
      gap: 5px;
      justify-content: center;
    }
    
    .benchmark-controls button {
      padding: 4px 8px;
      font-size: 11px;
      background: #2196F3;
      color: white;
      border: none;
      border-radius: 6px;
      cursor: pointer;
      transition: background 0.2s;
    }
    
    .benchmark-controls button:hover {
      background: #1976D2;
    }
    
    .audio-meter {
      display: flex;
      align-items: center;
      justify-content: center;
      margin-bottom: 20px;
      gap: 15px;
    }
    
    .meter-label {
      font-weight: 600;
      color: #555;
    }
    
    #meter { 
      width: 200px; 
      height: 8px; 
      background: rgba(0,0,0,0.1); 
      border-radius: 10px; 
      overflow: hidden;
      box-shadow: inset 0 2px 4px rgba(0,0,0,0.1);
    }
    
    #meter > i { 
      display: block; 
      height: 100%; 
      width: 0%; 
      background: linear-gradient(90deg, #4CAF50, #8BC34A);
      border-radius: 10px;
      transition: width 0.1s ease;
    }
    
    #status { 
      text-align: center;
      padding: 10px 20px;
      background: rgba(0,0,0,0.05);
      border-radius: 10px;
      margin-bottom: 20px;
      font-weight: 500;
      text-transform: capitalize;
    }
    
    /* Loader styles */
    #loader {
      text-align: center;
      margin: 20px 0;
      padding: 15px;
      background: linear-gradient(135deg, #e3f2fd, #bbdefb);
      border-radius: 12px;
      border-left: 4px solid #2196F3;
    }
    
    .spinner {
      display: inline-block;
      width: 20px;
      height: 20px;
      border: 3px solid rgba(33, 150, 243, 0.3);
      border-top: 3px solid #2196F3;
      border-radius: 50%;
      animation: spin 1s linear infinite;
      margin-left: 10px;
      vertical-align: middle;
    }
    
    @keyframes spin {
      0% { transform: rotate(0deg); }
      100% { transform: rotate(360deg); }
    }
    
    /* Conversation styles */
    #conversation {
      background: white;
      border-radius: 15px;
      padding: 20px;
      box-shadow: inset 0 2px 10px rgba(0,0,0,0.05);
      min-height: 300px;
      max-height: 500px;
      overflow-y: auto;
    }
    
    .conversation-header {
      text-align: center;
      margin-bottom: 20px;
      padding-bottom: 15px;
      border-bottom: 2px solid #f0f0f0;
    }
    
    .conversation-header h2 {
      font-size: 1.5rem;
      color: #333;
      font-weight: 600;
    }
    
    /* Message styles */
    .message {
      margin-bottom: 15px;
      animation: slideIn 0.3s ease;
    }
    
    @keyframes slideIn {
      from {
        opacity: 0;
        transform: translateY(10px);
      }
      to {
        opacity: 1;
        transform: translateY(0);
      }
    }
    
    .user-message {
      display: flex;
      justify-content: flex-end;
    }
    
    .gemini-message {
      display: flex;
      justify-content: flex-start;
    }
    
    .message-bubble {
      max-width: 70%;
      padding: 12px 18px;
      border-radius: 18px;
      position: relative;
    }
    
    .user-message .message-bubble {
      background: linear-gradient(135deg, #667eea, #764ba2);
      color: white;
      border-bottom-right-radius: 6px;
    }
    
    .gemini-message .message-bubble {
      background: linear-gradient(135deg, #f8f9fa, #e9ecef);
      color: #333;
      border-bottom-left-radius: 6px;
      border: 1px solid #e0e0e0;
    }
    
    .message-label {
      font-size: 11px;
      font-weight: 700;
      text-transform: uppercase;
      letter-spacing: 0.5px;
      margin-bottom: 5px;
      opacity: 0.8;
    }
    
    .user-message .message-label {
      color: rgba(255,255,255,0.9);
    }
    
    .gemini-message .message-label {
      color: #666;
    }
    
    .message-text {
      font-size: 15px;
      line-height: 1.5;
      word-wrap: break-word;
    }
    
    /* Scrollbar styling */
    #conversation::-webkit-scrollbar {
      width: 6px;
    }
    
    #conversation::-webkit-scrollbar-track {
      background: #f1f1f1;
      border-radius: 10px;
    }
    
    #conversation::-webkit-scrollbar-thumb {
      background: linear-gradient(135deg, #667eea, #764ba2);
      border-radius: 10px;
    }
    
    #conversation::-webkit-scrollbar-thumb:hover {
      background: #555;
    }
    
    /* Responsive design */
    @media (max-width: 600px) {
      .container {
        margin: 10px;
        padding: 20px;
      }
      
      .header h1 {
        font-size: 2rem;
      }
      
      .controls {
        flex-direction: column;
        align-items: center;
      }
      
      button {
        width: 100%;
        max-width: 200px;
      }
      
      .message-bubble {
        max-width: 85%;
      }
    }
  </style>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>AI Voice Assistant</h1>
      <p>Start a conversation with your intelligent AI companion</p>
    </div>
    
    <div class="controls">
      <button id="startBtn"><i class="fas fa-play"></i> Start Session</button>
      <button id="stopBtn" disabled><i class="fas fa-stop"></i> Stop Session</button>
      <button id="testBtn"><i class="fas fa-vial"></i> Test Message</button>
    </div>
    
    <div class="model-selector">
      <label for="modelSelect"><i class="fas fa-robot"></i> AI Model:</label>
      <select id="modelSelect">
        <option value="models/gemini-2.5-flash-preview-native-audio-dialog" selected>gemini-2.5-flash-preview-native-audio-dialog</option>
        <option value="models/gemini-2.5-flash-exp-native-audio-thinking-dialog">gemini-2.5-flash-exp-native-audio-thinking-dialog</option>
        <option value="models/gemini-live-2.5-flash-preview">gemini-live-2.5-flash-preview</option>
      </select>
    </div>
    
    <div class="audio-meter">
      <span class="meter-label"><i class="fas fa-microphone"></i> Audio Level:</span>
      <div id="meter"><i></i></div>
    </div>
    
    <div class="benchmark-info" id="benchmarkInfo">
      <div class="benchmark-stats">
        <span id="currentModel">No active session</span>
        <span id="interactionCount">Interactions: 0</span>
        <span id="avgResponse">Avg Response: --</span>
        <span id="avgAudio">Avg Audio: --</span>
      </div>
      <div class="benchmark-controls">
        <button onclick="window.benchmark.summary()" title="Show benchmark summary in console">📊 Summary</button>
        <button onclick="window.benchmark.export()" title="Export benchmark data">💾 Export</button>
        <button onclick="window.benchmark.clear()" title="Clear benchmark data">🗑️ Clear</button>
      </div>
    </div>
    
    <div id="status">🟢 idle</div>
    
    <div id="loader" style="display: none;">
      <span><i class="fas fa-robot"></i> AI is thinking...</span>
      <div class="spinner"></div>
    </div>
    
    <div id="conversation">
      <div class="conversation-header">
        <h2><i class="fas fa-comments"></i> Conversation</h2>
      </div>
      <div id="messages"></div>
    </div>
  </div>

<script>
/*
  Key design points:
  - Single WS connection reused for whole session.
  - Capture audio with AudioContext + ScriptProcessor and accumulate into ~CHUNK_MS chunks.
  - Send binary Int16 PCM chunks as ArrayBuffer. Precede them with a small JSON control message (depends on server).
  - Adaptive VAD: calibrate initial noise floor then detect voice start/stop using rolling RMS and thresholds.
  - Send `activityStart` when voice begins, `activityEnd` after silence timeout.
  - Play back server-sent audio (binary) by decoding and scheduling with AudioContext.
  - Keepalive pings and simple reconnect/backoff.
  NOTE: adjust SERVER_WS, and confirm server expects: 
        1) control messages as JSON text (e.g., {type: 'activityStart'}), and 
        2) raw PCM Int16 frames as binary. If your server expects Opus/webm blobs instead, switch capture method to MediaRecorder.
*/

// Dynamically build WebSocket URL for local and deployed environments
const SERVER_WS = (() => {
  let protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
  let host = window.location.host;
  // If running from file:// or localhost with no host, fallback to localhost:8000
  if (!host || host === '') {
    host = 'localhost:8000';
  }
  return `${protocol}//${host}/ws`;
})();
const CHUNK_MS = 200;                // send audio approx every X ms
const BUFFER_SIZE = 4096;            // ScriptProcessor buffer size
const SILENCE_DURATION_MS = 1000;    // silence required to send activityEnd
const CALIBRATE_MS = 1200;           // initial ambient noise calibration period
const SILENCE_THRESHOLD_MULT = 20.0;  // voice threshold = noiseFloor * mult (back to 5.0 with buffer protection)
const KEEPALIVE_INTERVAL_MS = 20000; // send ping every 20s

let ws = null;
let audioCtx = null;
let processor = null;
let mediaStream = null;
let sourceNode = null;
let sampleRate = 48000; // will update from audioCtx.sampleRate
let sendBuffer = [];    // accumulate Float32 samples
let sendBufferSize = 0;
let lastSendAt = 0;

// VAD vars
let playbackTime = 0;  // global tracker for sequential audio
let audioChunkCounter = 0; // Track audio chunks for debugging
let lastAudioDataHash = null; // Track duplicate audio data

// Add debugging flag to help identify double audio source
window.debugAudio = {
  disableDecoded: false,
  disableRawPCM: false,
  logLevel: 'normal' // 'normal' or 'verbose'
};
let benchmarkData = {
  currentModel: null,
  sessions: [],
  currentSession: null,
  startTime: null,
  firstResponseTime: null,
  firstAudioTime: null,
  messageCount: 0
};

function initBenchmark() {
  console.log('🔬 Benchmark system initialized');
  benchmarkData.sessions = [];
}

function startBenchmarkSession(model) {
  const sessionId = Date.now();
  benchmarkData.currentSession = {
    id: sessionId,
    model: model,
    startTime: Date.now(),
    interactions: [],
    totalMessages: 0,
    totalResponseTime: 0,
    totalAudioTime: 0,
    averageResponseTime: 0,
    averageAudioTime: 0
  };
  benchmarkData.currentModel = model;
  benchmarkData.sessions.push(benchmarkData.currentSession);
  
  console.log(`🎯 Started benchmark session for model: ${model}`);
  console.log(`📊 Session ID: ${sessionId}`);
  
  updateBenchmarkUI();
}

function startInteractionTimer() {
  console.log(`🐛 DEBUG - startInteractionTimer called`);
  benchmarkData.startTime = Date.now();
  benchmarkData.firstResponseTime = null;
  benchmarkData.firstAudioTime = null;
  benchmarkData.messageCount++;
  console.log(`🐛 DEBUG - Timer started - startTime: ${benchmarkData.startTime}, messageCount: ${benchmarkData.messageCount}`);
}

function recordFirstResponse() {
  console.log(`🐛 DEBUG - recordFirstResponse called`);
  console.log(`🐛 DEBUG - firstResponseTime exists: ${!!benchmarkData.firstResponseTime}`);
  console.log(`🐛 DEBUG - startTime exists: ${!!benchmarkData.startTime}`);
  
  // Fallback: if startTime is null, start it now (in case VAD didn't trigger)
  if (!benchmarkData.startTime) {
    console.log(`🐛 DEBUG - startTime was null, starting timer now as fallback`);
    benchmarkData.startTime = Date.now();
    benchmarkData.messageCount++;
  }
  
  if (!benchmarkData.firstResponseTime && benchmarkData.startTime) {
    benchmarkData.firstResponseTime = Date.now() - benchmarkData.startTime;
    console.log(`⚡ First response: ${(benchmarkData.firstResponseTime / 1000).toFixed(2)}s`);
    console.log(`🐛 DEBUG - Start time: ${benchmarkData.startTime}, Current time: ${Date.now()}, Diff: ${benchmarkData.firstResponseTime}`);
  } else {
    console.log(`🐛 DEBUG - recordFirstResponse skipped - firstResponseTime: ${benchmarkData.firstResponseTime}, startTime: ${benchmarkData.startTime}`);
  }
}

function recordFirstAudio() {
  console.log(`🐛 DEBUG - recordFirstAudio called`);
  console.log(`🐛 DEBUG - firstAudioTime exists: ${!!benchmarkData.firstAudioTime}`);
  console.log(`🐛 DEBUG - startTime exists: ${!!benchmarkData.startTime}`);
  
  // Fallback: if startTime is null, start it now (in case VAD didn't trigger)
  if (!benchmarkData.startTime) {
    console.log(`🐛 DEBUG - startTime was null, starting timer now as fallback`);
    benchmarkData.startTime = Date.now();
    benchmarkData.messageCount++;
  }
  
  if (!benchmarkData.firstAudioTime && benchmarkData.startTime) {
    benchmarkData.firstAudioTime = Date.now() - benchmarkData.startTime;
    console.log(`🎵 First audio: ${(benchmarkData.firstAudioTime / 1000).toFixed(2)}s`);
    console.log(`🐛 DEBUG - Start time: ${benchmarkData.startTime}, Current time: ${Date.now()}, Diff: ${benchmarkData.firstAudioTime}`);
  } else {
    console.log(`🐛 DEBUG - recordFirstAudio skipped - firstAudioTime: ${benchmarkData.firstAudioTime}, startTime: ${benchmarkData.startTime}`);
  }
}

function endInteraction() {
  console.log(`🐛 DEBUG - endInteraction called`);
  console.log(`🐛 DEBUG - benchmarkData.currentSession:`, benchmarkData.currentSession);
  console.log(`🐛 DEBUG - benchmarkData.startTime:`, benchmarkData.startTime);
  console.log(`🐛 DEBUG - benchmarkData.firstResponseTime:`, benchmarkData.firstResponseTime);
  console.log(`🐛 DEBUG - benchmarkData.firstAudioTime:`, benchmarkData.firstAudioTime);
  
  if (benchmarkData.currentSession && benchmarkData.startTime) {
    const totalTime = Date.now() - benchmarkData.startTime;
    
    const interaction = {
      messageIndex: benchmarkData.messageCount,
      timestamp: new Date().toISOString(),
      responseTime: benchmarkData.firstResponseTime,
      audioTime: benchmarkData.firstAudioTime,
      totalTime: totalTime
    };
    
    benchmarkData.currentSession.interactions.push(interaction);
    benchmarkData.currentSession.totalMessages++;
    
    if (benchmarkData.firstResponseTime) {
      benchmarkData.currentSession.totalResponseTime += benchmarkData.firstResponseTime;
      benchmarkData.currentSession.averageResponseTime = 
        benchmarkData.currentSession.totalResponseTime / benchmarkData.currentSession.totalMessages;
    }
    
    if (benchmarkData.firstAudioTime) {
      benchmarkData.currentSession.totalAudioTime += benchmarkData.firstAudioTime;
      benchmarkData.currentSession.averageAudioTime = 
        benchmarkData.currentSession.totalAudioTime / benchmarkData.currentSession.totalMessages;
    }
    
    console.log(`📈 Interaction ${benchmarkData.messageCount} completed:`, interaction);
    console.log(`📊 Session averages - Response: ${(benchmarkData.currentSession.averageResponseTime / 1000).toFixed(2)}s, Audio: ${(benchmarkData.currentSession.averageAudioTime / 1000).toFixed(2)}s`);
    
    // Update UI
    updateBenchmarkUI();
    
    // Reset for next interaction
    benchmarkData.startTime = null;
    benchmarkData.firstResponseTime = null;
    benchmarkData.firstAudioTime = null;
  }
}

function endBenchmarkSession() {
  if (benchmarkData.currentSession) {
    benchmarkData.currentSession.endTime = Date.now();
    benchmarkData.currentSession.duration = benchmarkData.currentSession.endTime - benchmarkData.currentSession.startTime;
    
    console.log(`🏁 Benchmark session ended for ${benchmarkData.currentModel}`);
    console.log(`📊 Session summary:`, {
      model: benchmarkData.currentSession.model,
      duration: `${(benchmarkData.currentSession.duration / 1000).toFixed(1)}s`,
      totalMessages: benchmarkData.currentSession.totalMessages,
      averageResponseTime: `${(benchmarkData.currentSession.averageResponseTime / 1000).toFixed(2)}s`,
      averageAudioTime: `${(benchmarkData.currentSession.averageAudioTime / 1000).toFixed(2)}s`,
      interactions: benchmarkData.currentSession.interactions
    });
    
    benchmarkData.currentSession = null;
    benchmarkData.currentModel = null;
    benchmarkData.messageCount = 0;
    
    // Update UI
    updateBenchmarkUI();
  }
}

function exportBenchmarkData() {
  const exportData = {
    timestamp: new Date().toISOString(),
    totalSessions: benchmarkData.sessions.length,
    sessions: benchmarkData.sessions,
    summary: generateBenchmarkSummary()
  };
  
  console.log('📊 COMPLETE BENCHMARK DATA:', exportData);
  
  // Also download as JSON file
  const dataStr = JSON.stringify(exportData, null, 2);
  const dataBlob = new Blob([dataStr], {type: 'application/json'});
  const url = URL.createObjectURL(dataBlob);
  const link = document.createElement('a');
  link.href = url;
  link.download = `benchmark_${new Date().toISOString().replace(/:/g, '-')}.json`;
  link.click();
  URL.revokeObjectURL(url);
  
  return exportData;
}

function generateBenchmarkSummary() {
  const modelStats = {};
  
  benchmarkData.sessions.forEach(session => {
    if (!modelStats[session.model]) {
      modelStats[session.model] = {
        sessions: 0,
        totalMessages: 0,
        totalResponseTime: 0,
        totalAudioTime: 0,
        responseTimes: [],
        audioTimes: []
      };
    }
    
    const stats = modelStats[session.model];
    stats.sessions++;
    stats.totalMessages += session.totalMessages;
    stats.totalResponseTime += session.totalResponseTime;
    stats.totalAudioTime += session.totalAudioTime;
    
    session.interactions.forEach(interaction => {
      if (interaction.responseTime) stats.responseTimes.push(interaction.responseTime);
      if (interaction.audioTime) stats.audioTimes.push(interaction.audioTime);
    });
  });
  
  // Calculate statistics for each model
  Object.keys(modelStats).forEach(model => {
    const stats = modelStats[model];
    stats.avgResponseTime = stats.responseTimes.length > 0 ? 
      stats.responseTimes.reduce((a, b) => a + b, 0) / stats.responseTimes.length : 0;
    stats.avgAudioTime = stats.audioTimes.length > 0 ? 
      stats.audioTimes.reduce((a, b) => a + b, 0) / stats.audioTimes.length : 0;
    
    if (stats.responseTimes.length > 0) {
      stats.responseTimes.sort((a, b) => a - b);
      stats.minResponseTime = stats.responseTimes[0];
      stats.maxResponseTime = stats.responseTimes[stats.responseTimes.length - 1];
      stats.medianResponseTime = stats.responseTimes[Math.floor(stats.responseTimes.length / 2)];
      
      // Convert to seconds for display
      stats.avgResponseTimeSeconds = (stats.avgResponseTime / 1000).toFixed(2);
      stats.minResponseTimeSeconds = (stats.minResponseTime / 1000).toFixed(2);
      stats.maxResponseTimeSeconds = (stats.maxResponseTime / 1000).toFixed(2);
      stats.medianResponseTimeSeconds = (stats.medianResponseTime / 1000).toFixed(2);
    }
    
    if (stats.audioTimes.length > 0) {
      stats.avgAudioTimeSeconds = (stats.avgAudioTime / 1000).toFixed(2);
    }
  });
  
  return modelStats;
}

function updateBenchmarkUI() {
  const currentModelEl = document.getElementById('currentModel');
  const interactionCountEl = document.getElementById('interactionCount');
  const avgResponseEl = document.getElementById('avgResponse');
  const avgAudioEl = document.getElementById('avgAudio');
  
  if (benchmarkData.currentSession) {
    const session = benchmarkData.currentSession;
    const modelName = session.model.replace('models/', '').substring(0, 20) + '...';
    
    currentModelEl.textContent = `Model: ${modelName}`;
    interactionCountEl.textContent = `Interactions: ${session.totalMessages}`;
    
    if (session.averageResponseTime > 0) {
      avgResponseEl.textContent = `Avg Response: ${(session.averageResponseTime / 1000).toFixed(2)}s`;
    } else {
      avgResponseEl.textContent = `Avg Response: --`;
    }
    
    if (session.averageAudioTime > 0) {
      avgAudioEl.textContent = `Avg Audio: ${(session.averageAudioTime / 1000).toFixed(2)}s`;
    } else {
      avgAudioEl.textContent = `Avg Audio: --`;
    }
  } else {
    currentModelEl.textContent = 'No active session';
    interactionCountEl.textContent = 'Interactions: 0';
    avgResponseEl.textContent = 'Avg Response: --';
    avgAudioEl.textContent = 'Avg Audio: --';
  }
}

// Add benchmark controls to console
window.benchmark = {
  export: exportBenchmarkData,
  summary: () => {
    console.log('📊 BENCHMARK SUMMARY:', generateBenchmarkSummary());
    return generateBenchmarkSummary();
  },
  clear: () => {
    benchmarkData.sessions = [];
    console.log('🗑️ Benchmark data cleared');
    updateBenchmarkUI();
  },
  current: () => benchmarkData.currentSession
};
let activeAudioSources = [];  // track active audio sources for interruption
let audioSourceCounter = 0;   // assign unique IDs to audio sources for debugging
let noiseFloor = 0;
let calibrated = false;
let calibrationStart = null;
let speaking = false;
let silenceStart = null;
let currentUserMessage = ""; // Track current user speech
let lastUserMessageElement = null; // Track the last user message element for updates
let lastGeminiMessageElement = null; // Track the last Gemini message element for streaming updates
let currentGeminiMessage = ""; // Track current Gemini response being built
let lastAudioStartTime = 0; // Track when Gemini audio started
const AUDIO_START_BUFFER_MS = 500; // Wait 500ms after audio starts before allowing interrupts

// UI
const startBtn = document.getElementById('startBtn');
const stopBtn = document.getElementById('stopBtn');
const testBtn = document.getElementById('testBtn');
const modelSelect = document.getElementById('modelSelect');
const statusEl = document.getElementById('status');
const meterBar = document.querySelector('#meter > i');
const loaderEl = document.getElementById('loader');
const messagesEl = document.getElementById('messages');

console.log('[INIT] DOM elements found:', {
  startBtn: !!startBtn,
  stopBtn: !!stopBtn,
  testBtn: !!testBtn,
  statusEl: !!statusEl,
  meterBar: !!meterBar,
  loaderEl: !!loaderEl,
  messagesEl: !!messagesEl
});

// Initialize benchmark system
initBenchmark();
updateBenchmarkUI();

startBtn.onclick = startSession;
stopBtn.onclick = stopSession;
testBtn.onclick = () => {
  console.log('[TEST] Adding test messages');
  addMessage('Test user message', true);
  addMessage('Test Gemini response', false);
};

// Handle model changes during active session
modelSelect.onchange = () => {
  const newModel = modelSelect.value;
  console.log('[MODEL] Model changed to:', newModel);
  
  // If session is active (stop button is enabled), restart with new model
  if (!stopBtn.disabled) {
    console.log('[MODEL] Session active, restarting with new model...');
    logStatus(`Switching to ${newModel.replace('models/', '')}...`);
    
    // Stop current session and start new one with selected model
    stopSession();
    setTimeout(() => {
      startSession();
    }, 500); // Small delay to ensure clean restart
  }
};

function logStatus(s) {
  statusEl.textContent = s;
}

function showLoader() {
  loaderEl.style.display = 'block';
}

function hideLoader() {
  loaderEl.style.display = 'none';
}

function addMessage(text, isUser = false, updateLast = false) {
  console.log(`[addMessage] Called with: text="${text}", isUser=${isUser}, updateLast=${updateLast}`);
  console.log(`[addMessage] messagesEl:`, messagesEl);
  
  if (updateLast && isUser && lastUserMessageElement) {
    // Update the existing user message
    const textDiv = lastUserMessageElement.querySelector('.message-bubble .message-text');
    if (textDiv) {
      textDiv.textContent = text;
      console.log(`[addMessage] Updated existing user message to: "${text}"`);
    }
    return;
  }
  
  if (updateLast && !isUser && lastGeminiMessageElement) {
    // Update the existing Gemini message (for streaming)
    const textDiv = lastGeminiMessageElement.querySelector('.message-bubble .message-text');
    if (textDiv) {
      textDiv.textContent = text;
      console.log(`[addMessage] Updated existing Gemini message to: "${text}"`);
      // Auto-scroll to bottom
      messagesEl.scrollTop = messagesEl.scrollHeight;
    }
    return;
  }
  
  const messageDiv = document.createElement('div');
  messageDiv.className = `message ${isUser ? 'user-message' : 'gemini-message'}`;
  
  const bubbleDiv = document.createElement('div');
  bubbleDiv.className = 'message-bubble';
  
  const labelDiv = document.createElement('div');
  labelDiv.className = 'message-label';
  labelDiv.textContent = isUser ? '👤 You' : '🤖 AI Assistant';
  
  const textDiv = document.createElement('div');
  textDiv.className = 'message-text';
  textDiv.textContent = text;
  
  bubbleDiv.appendChild(labelDiv);
  bubbleDiv.appendChild(textDiv);
  messageDiv.appendChild(bubbleDiv);
  
  console.log('[addMessage] Created messageDiv:', messageDiv);
  console.log('[addMessage] messagesEl before appendChild:', messagesEl);
  
  messagesEl.appendChild(messageDiv);
  
  console.log('[addMessage] messagesEl after appendChild, children count:', messagesEl.children.length);
  console.log(`[addMessage] Added new message: "${text}" (${isUser ? 'user' : 'gemini'})`);
  console.log(`[addMessage] Total messages now:`, messagesEl.children.length);
  
  if (isUser) {
    lastUserMessageElement = messageDiv;
    console.log(`[addMessage] Set lastUserMessageElement`);
  } else {
    lastGeminiMessageElement = messageDiv;
    console.log(`[addMessage] Set lastGeminiMessageElement`);
  }
  
  // Auto-scroll to bottom
  messagesEl.scrollTop = messagesEl.scrollHeight;
}

function clearMessages() {
  messagesEl.innerHTML = '';
  lastUserMessageElement = null;
  lastGeminiMessageElement = null;
  currentGeminiMessage = "";
}

// Helper function to track audio sources
function trackAudioSource(src, type) {
  const id = ++audioSourceCounter;
  src._debugId = id;
  
  // No audio source limiting - let all audio play naturally
  activeAudioSources.push(src);
  
  // Track when audio starts for interrupt buffer logic
  if (activeAudioSources.length === 1) { // First audio source
    lastAudioStartTime = Date.now();
    console.log(`[AUDIO] First audio started, setting buffer time: ${lastAudioStartTime}`);
  }
  
  console.log(`[AUDIO] Started ${type} audio (ID: ${id}, ${activeAudioSources.length} total sources)`);
  
  src.onended = () => {
    const index = activeAudioSources.indexOf(src);
    if (index > -1) {
      activeAudioSources.splice(index, 1);
      console.log(`[AUDIO] ${type} audio ended naturally (ID: ${id}, ${activeAudioSources.length} remaining)`);
    }
  };
  
  return src;
}

// --- WebSocket helpers ---
let pingTimer = null;
let reconnectTimer = null;
let reconnectAttempts = 0;

function openWs() {
  if (ws && ws.readyState === WebSocket.OPEN) return;
  ws = new WebSocket(SERVER_WS);
  ws.binaryType = 'arraybuffer';

  ws.onopen = () => {
    console.log('WS open');
    reconnectAttempts = 0;
    logStatus('connected');
    startKeepalive();
    // optionally send session-start control
    sendJson({ type: 'session_start', sampleRate });
  };

  ws.onmessage = async (ev) => {
    console.log('[CLIENT] Raw message received:', ev.data);
    console.log('[CLIENT] Message type:', typeof ev.data);
    
    // If server sends text JSON
    if (typeof ev.data === 'string') {
      try {
        const msg = JSON.parse(ev.data);
        console.log('[CLIENT] Parsed JSON message:', msg);
        console.log('[CLIENT] Message type field:', msg.type);
        console.log('[CLIENT] Message text field:', msg.text);
        
        // Handle interruption signal from server
        if (msg.type === 'interrupt' && msg.message === 'stop_playback') {
          console.log('[CLIENT] Handling interrupt signal');
          stopAllAudioPlayback();
          hideLoader(); // Hide loader if interruption occurs
          return;
        }
        
        // Handle turn completion signal from server
        if (msg.type === 'turn_complete') {
          console.log('[CLIENT] Gemini turn completed, resetting message tracking');
          
          // End benchmark interaction
          endInteraction();
          
          // Reset message tracking for next response
          lastGeminiMessageElement = null;
          lastUserMessageElement = null; // Also reset user message element for next turn
          // Note: don't reset currentGeminiMessage here as it might still be displayed
          return;
        }
        
        // Handle text responses from Gemini
        if (msg.type === 'text' && msg.text) {
          console.log('[CLIENT] Handling text response from Gemini:', msg.text);
          console.log('[CLIENT] messagesEl exists:', !!messagesEl);
          console.log('[CLIENT] lastGeminiMessageElement exists:', !!lastGeminiMessageElement);
          
          // If this is the start of a new Gemini response, reset the message
          if (!lastGeminiMessageElement) {
            console.log('[CLIENT] Starting new Gemini response, resetting message');
            currentGeminiMessage = "";
          }
          
          // Append this chunk to the current Gemini message
          currentGeminiMessage += msg.text;
          console.log('[CLIENT] Current Gemini message:', currentGeminiMessage);
          
          if (!lastGeminiMessageElement) {
            // Create new message if none exists - this is the first response
            console.log('[CLIENT] Creating new Gemini message');
            console.log('🐛 DEBUG - About to call recordFirstResponse()');
            recordFirstResponse(); // Record benchmark timing
            addMessage(currentGeminiMessage, false);
            console.log('[CLIENT] After addMessage - lastGeminiMessageElement:', !!lastGeminiMessageElement);
          } else {
            // Update existing message with accumulated text
            console.log('[CLIENT] Updating existing Gemini message');
            addMessage(currentGeminiMessage, false, true);
          }
          
          console.log('[CLIENT] Called addMessage for Gemini response');
          return;
        }
        
        // Handle transcriptions from server
        if (msg.type === 'transcript') {
          console.log(`[CLIENT] Handling ${msg.source} transcript:`, msg.text);
          
          // Make sure no TTS is accidentally triggered
          if (window.speechSynthesis) {
            window.speechSynthesis.cancel(); // Stop any accidental TTS
          }
          
          if (msg.source === 'input') {
            // User speech transcription - update in real-time
            currentUserMessage = msg.text;
            if (speaking) {
              // If user is still speaking, show live transcription
              if (!lastUserMessageElement) {
                console.log('[CLIENT] Adding new user transcript message');
                addMessage(msg.text, true);
              } else {
                console.log('[CLIENT] Updating existing user transcript message');
                addMessage(msg.text, true, true); // Update existing message
              }
            }
          } else if (msg.source === 'output') {
            // Gemini speech transcription - display this as the text response
            console.log('[CLIENT] Gemini speech transcript:', msg.text);
            
            // If this is the start of a new Gemini response, reset the message
            if (!lastGeminiMessageElement) {
              console.log('[CLIENT] Starting new Gemini response from transcript, resetting message');
              currentGeminiMessage = "";
            }
            
            // Append this chunk to the current Gemini message
            currentGeminiMessage += msg.text;
            console.log('[CLIENT] Current Gemini message from transcript:', currentGeminiMessage);
            
            if (!lastGeminiMessageElement) {
              // Create new message if none exists - this is the first response
              console.log('[CLIENT] Creating new Gemini message from transcript');
              recordFirstResponse(); // Record benchmark timing
              addMessage(currentGeminiMessage, false);
              console.log('[CLIENT] After addMessage - lastGeminiMessageElement:', !!lastGeminiMessageElement);
            } else {
              // Update existing message with accumulated text
              console.log('[CLIENT] Updating existing Gemini message from transcript');
              addMessage(currentGeminiMessage, false, true);
            }
          }
          return;
        }
        
        // handle other control text messages as needed
        console.log('[CLIENT] Unhandled JSON message type:', msg.type, msg);
        return;
      } catch (e) {
        console.log('[CLIENT] Failed to parse JSON:', e);
        console.log('[CLIENT] Non-JSON text received:', ev.data);
        return;
      }
    }

    // binary: assume audio bytes (raw PCM16 24kHz from Gemini) — decode & play
    if (ev.data instanceof ArrayBuffer) {
      audioChunkCounter++;
      
      // Simple hash to detect duplicate audio chunks
      const audioHash = ev.data.byteLength + '_' + (audioChunkCounter % 10);
      if (audioHash === lastAudioDataHash && ev.data.byteLength < 15000) { // Only check small chunks
        console.log(`[AUDIO] Skipping potential duplicate chunk #${audioChunkCounter}`);
        return;
      }
      lastAudioDataHash = audioHash;
      
      // Skip only extremely tiny chunks (very minimal filtering now)
      if (ev.data.byteLength < 50) {
        console.log(`[AUDIO] Skipping extremely tiny chunk #${audioChunkCounter} (${ev.data.byteLength} bytes)`);
        return;
      }
      
      console.log(`[AUDIO] Received audio chunk #${audioChunkCounter}, size: ${ev.data.byteLength} bytes`);
      hideLoader(); // Hide loader when audio starts arriving
      
      const audioData = ev.data;
      let audioProcessed = false; // Flag to prevent double processing
      
      try {
        // Try decodeAudioData first — this assumes the server sent a valid encoded audio container (e.g. WAV/Opus)
        audioCtx.decodeAudioData(audioData.slice(0)).then(decoded => {
          if (!audioProcessed && !window.debugAudio.disableDecoded) {
            audioProcessed = true;
            console.log(`[AUDIO] Chunk #${audioChunkCounter} successfully decoded audio, playing via decodeAudioData`);
            playDecodedAudio(decoded);
          } else if (window.debugAudio.disableDecoded) {
            console.log(`[AUDIO] Chunk #${audioChunkCounter} decoded audio DISABLED for debugging`);
          }
        }).catch(err => {
          if (!audioProcessed && !window.debugAudio.disableRawPCM) {
            audioProcessed = true;
            // Fallback: treat as raw PCM16LE 24000Hz mono
            console.log(`[AUDIO] Chunk #${audioChunkCounter} decodeAudioData failed — playing as raw PCM16 24kHz`, err.message);
            playRawPCM16(audioData, 24000);
          } else if (window.debugAudio.disableRawPCM) {
            console.log(`[AUDIO] Chunk #${audioChunkCounter} raw PCM audio DISABLED for debugging`);
          }
        });
      } catch (err) {
        if (!audioProcessed) {
          audioProcessed = true;
          console.log(`[AUDIO] Chunk #${audioChunkCounter} audio context error, playing as raw PCM16`, err.message);
          playRawPCM16(audioData, 24000);
        }
      }
    }
  };

  ws.onclose = (e) => {
    console.log('WS closed', e);
    stopKeepalive();
    hideLoader(); // Hide loader if connection is lost
    logStatus('disconnected');
    scheduleReconnect();
  };

  ws.onerror = (e) => {
    console.error('WS error', e);
    hideLoader(); // Hide loader on error
  };
}

// Play raw PCM16LE mono at given sample rate
function playRawPCM16(buffer, sampleRate = 24000) {
  console.log(`[AUDIO] playRawPCM16 called - buffer size: ${buffer.byteLength} bytes, sample rate: ${sampleRate}`);
  
  if (!audioCtx) audioCtx = new (window.AudioContext || window.webkitAudioContext)();
  const int16 = new Int16Array(buffer);
  const float32 = new Float32Array(int16.length);
  for (let i = 0; i < int16.length; i++) {
    float32[i] = int16[i] / 32768;
  }
  const audioBuffer = audioCtx.createBuffer(1, float32.length, sampleRate);
  audioBuffer.getChannelData(0).set(float32);
  const src = audioCtx.createBufferSource();
  src.buffer = audioBuffer;
  src.connect(audioCtx.destination);
  
  // Track this audio source for potential interruption
  trackAudioSource(src, 'raw-PCM16');
  
  // Record first audio timing for benchmark
  recordFirstAudio();
  
  // Simple sequential scheduling
  const now = audioCtx.currentTime;
  if (playbackTime < now) {
    playbackTime = now;
  }
  
  console.log(`[AUDIO] Playing raw PCM16 at ${playbackTime.toFixed(3)}s, duration: ${audioBuffer.duration.toFixed(3)}s`);
  src.start(playbackTime);
  playbackTime += audioBuffer.duration;
}

function scheduleReconnect() {
  if (reconnectTimer) return;
  reconnectAttempts++;
  const backoff = Math.min(30000, 1000 * Math.pow(1.5, reconnectAttempts));
  console.log('reconnect in', backoff);
  reconnectTimer = setTimeout(() => {
    reconnectTimer = null;
    openWs();
  }, backoff);
}

function startKeepalive(){
  stopKeepalive();
  pingTimer = setInterval(() => {
    if (ws && ws.readyState === WebSocket.OPEN) {
      try { ws.send(JSON.stringify({type:'ping', ts: Date.now()})); }
      catch(e){}
    }
  }, KEEPALIVE_INTERVAL_MS);
}
function stopKeepalive(){ if (pingTimer) { clearInterval(pingTimer); pingTimer = null; } }
function sendJson(obj){ if (ws && ws.readyState === WebSocket.OPEN) ws.send(JSON.stringify(obj)); }

// --- Audio capture, chunking, encoding to Int16 ---
function startAudio() {
  audioCtx = new (window.AudioContext || window.webkitAudioContext)();
  sampleRate = audioCtx.sampleRate;
  return navigator.mediaDevices.getUserMedia({audio:true})
    .then(stream => {
      mediaStream = stream;
      sourceNode = audioCtx.createMediaStreamSource(stream);

      // ScriptProcessorNode for capture (simple, widely supported)
      processor = audioCtx.createScriptProcessor(BUFFER_SIZE, 1, 1);
      sourceNode.connect(processor);
      processor.connect(audioCtx.destination); // required in some browsers

      // reset send buffer
      sendBuffer = [];
      sendBufferSize = 0;
      calibrated = false;
      noiseFloor = 0;
      calibrationStart = performance.now();

      processor.onaudioprocess = onAudioProcess;
      logStatus('listening for voice...');
    });
}

function stopAudioNodes() {
  if (processor) { processor.disconnect(); processor.onaudioprocess = null; processor = null; }
  if (sourceNode) { try { sourceNode.disconnect(); } catch(e){} sourceNode = null; }
  if (mediaStream) { mediaStream.getTracks().forEach(t => t.stop()); mediaStream = null; }
  if (audioCtx) { try { audioCtx.close(); } catch(e){} audioCtx = null; }
}

// accumulate frames, send when we cross CHUNK_MS window
function onAudioProcess(ev) {
  const input = ev.inputBuffer.getChannelData(0);
  const now = performance.now();

  // compute RMS and update UI meter
  let sum = 0;
  for (let i=0;i<input.length;i++){ sum += input[i]*input[i]; }
  const rms = Math.sqrt(sum / input.length);
  updateMeter(rms);

  // calibration
  if (!calibrated) {
    if (now - calibrationStart < CALIBRATE_MS) {
      noiseFloor = noiseFloor === 0 ? rms : (noiseFloor * 0.95 + rms * 0.05);
    } else {
      calibrated = true;
      noiseFloor = Math.max(1e-6, noiseFloor); // avoid zero
      console.log('calibrated noiseFloor', noiseFloor);
    }
  }

  // VAD logic - use moderately higher threshold while Gemini audio is playing
  const isGeminiSpeaking = activeAudioSources.length > 0;
  const baseThreshold = Math.max(1e-6, noiseFloor * SILENCE_THRESHOLD_MULT);
  const voiceThreshold = isGeminiSpeaking ? baseThreshold * 2 : baseThreshold; // 2x higher threshold while Gemini speaks (instead of 4x)
  const currentTime = Date.now();
  
  if (calibrated && rms > voiceThreshold) {
    // voice present and above appropriate threshold
    silenceStart = null;
    if (!speaking) {
      speaking = true;
      
      console.log(`[VAD] Voice detected - RMS: ${rms.toFixed(4)}, Threshold: ${voiceThreshold.toFixed(4)} (${isGeminiSpeaking ? 'high - Gemini speaking' : 'normal'}), Audio sources: ${activeAudioSources.length}`);
      
      // Reset Gemini message tracking for new turn
      currentGeminiMessage = "";
      lastGeminiMessageElement = null;
      
      console.log('🐛 DEBUG - Voice activity detected, starting interaction');
      
      // Create initial user message placeholder
      addMessage('...', true);
      
      sendJson({ type: 'activityStart', ts: currentTime });
      console.log('>>> activityStart (with adaptive threshold)');
      
      // Start benchmark timer for this interaction
      startInteractionTimer();
      logStatus('speaking');
    }
  } else {
    // low volume
    if (speaking) {
      if (!silenceStart) silenceStart = now;
      else if (now - silenceStart >= SILENCE_DURATION_MS) {
        // end of user turn
        speaking = false;
        silenceStart = null;
        sendJson({ type: 'activityEnd', ts: Date.now() });
        console.log('>>> activityEnd');
        
        // Finalize user message with transcription or fallback
        if (currentUserMessage && currentUserMessage !== "...") {
          // We have a real transcription, make sure it's displayed
          if (lastUserMessageElement) {
            addMessage(currentUserMessage, true, true);
          } else {
            addMessage(currentUserMessage, true);
          }
        } else if (lastUserMessageElement) {
          // No transcription received, show fallback
          addMessage('[Voice message]', true, true);
        } else {
          // Fallback if no message element exists
          addMessage('[Voice message]', true);
        }
        
        currentUserMessage = "";
        
        logStatus('idle (waiting for response...)');
        
        // Reset Gemini message tracking for new response
        currentGeminiMessage = "";
        lastGeminiMessageElement = null;
        
        showLoader(); // Show loader when waiting for Gemini's response
      }
    } else {
      // not speaking, optionally adapt noise floor slowly
      if (calibrated) {
        // adapt noise floor slowly so environment drift is handled
        noiseFloor = noiseFloor * 0.999 + rms * 0.001;
      }
    }
  }

  // Only send audio chunks if currently speaking (between activityStart and activityEnd)
  if (speaking) {
    // copy into accumulator
    // keep a typed copy to avoid referencing float32 view later
    sendBuffer.push(new Float32Array(input));
    sendBufferSize += input.length;

    // check if enough samples to send chunk (CHUNK_MS)
    const samplesPerMs = sampleRate / 1000;
    const targetSamples = Math.floor(samplesPerMs * CHUNK_MS);

    if (sendBufferSize >= targetSamples) {
      // merge into single Float32Array
      const out = new Float32Array(sendBufferSize);
      let offset = 0;
      for (let buf of sendBuffer) {
        out.set(buf, offset);
        offset += buf.length;
      }
      // reset accumulator
      sendBuffer = [];
      sendBufferSize = 0;

      // convert to int16
      const int16 = floatTo16BitPCM(out);

      // send a small header JSON before the binary so server can interpret if needed
      if (ws && ws.readyState === WebSocket.OPEN) {
        // header includes type and sampleRate so server can decode appropriately
        // try { ws.send(JSON.stringify({ type:'input_audio_chunk', encoding:'pcm_s16', sampleRate: sampleRate, ts: Date.now() })); } catch(e){}
        try { ws.send(int16.buffer); } catch(e){ console.error('send binary fail', e); }
        lastSendAt = Date.now();
      }
    }
  }
}

// float32 -> Int16 (little endian)
function floatTo16BitPCM(float32Array) {
  const l = float32Array.length;
  const buffer = new ArrayBuffer(l * 2);
  const view = new DataView(buffer);
  let offset = 0;
  for (let i = 0; i < l; i++, offset += 2) {
    let s = Math.max(-1, Math.min(1, float32Array[i]));
    view.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7FFF, true);
  }
  return new Int16Array(buffer);
}

// --- Playback of decoded AudioBuffer ---
function stopAllAudioPlayback() {
  // Stop all currently playing audio sources
  console.log(`[AUDIO] Attempting to stop ${activeAudioSources.length} active audio sources`);
  
  let stoppedCount = 0;
  const sourcesToStop = [...activeAudioSources]; // Create copy to avoid array modification issues
  
  for (let i = 0; i < sourcesToStop.length; i++) {
    const source = sourcesToStop[i];
    try {
      // Check if source is still playable
      if (source && typeof source.stop === 'function') {
        source.stop(0); // Stop immediately
        stoppedCount++;
        console.log(`[AUDIO] Stopped audio source ID: ${source._debugId || 'unknown'}`);
      }
    } catch (e) {
      // Source might already be stopped or finished
      console.log(`[AUDIO] Source ID ${source._debugId || 'unknown'} already stopped:`, e.message);
    }
  }
  
  // Clear the entire array
  activeAudioSources.length = 0;
  
  console.log(`[AUDIO] Successfully stopped ${stoppedCount} audio sources, cleared tracking array`);
  
  // Reset the sequential playback time to current time
  if (audioCtx) {
    const oldPlaybackTime = playbackTime;
    playbackTime = audioCtx.currentTime;
    console.log(`[AUDIO] Reset playback time from ${oldPlaybackTime.toFixed(3)} to ${playbackTime.toFixed(3)}`);
  }
}

function playDecodedAudio(audioBuffer) {
  console.log(`[AUDIO] playDecodedAudio called - duration: ${audioBuffer.duration.toFixed(3)}s, channels: ${audioBuffer.numberOfChannels}, sample rate: ${audioBuffer.sampleRate}`);
  
  if (!audioCtx) audioCtx = new (window.AudioContext || window.webkitAudioContext)();
  const src = audioCtx.createBufferSource();
  src.buffer = audioBuffer;
  src.connect(audioCtx.destination);
  
  // Track this audio source for potential interruption
  trackAudioSource(src, 'decoded');
  
  // Record first audio timing for benchmark
  recordFirstAudio();
  
  // Simple sequential scheduling
  const now = audioCtx.currentTime;
  if (playbackTime < now) {
    playbackTime = now;
  }
  
  console.log(`[AUDIO] Playing decoded audio at ${playbackTime.toFixed(3)}s, duration: ${audioBuffer.duration.toFixed(3)}s`);
  src.start(playbackTime);
  playbackTime += audioBuffer.duration;
}

// --- UI helpers ---
function updateMeter(rms) {
  // convert rms to 0..1 scale (rough)
  // scale by 10x so quiet voices are visible; tune as required
  const v = Math.min(1, Math.sqrt(rms) * 10);
  meterBar.style.width = `${(v*100).toFixed(1)}%`;
}

// --- Control flow: start/stop session ---
async function startSession() {
  startBtn.disabled = true;
  stopBtn.disabled = false;
  hideLoader(); // Make sure loader starts hidden
  clearMessages(); // Clear previous conversation
  stopAllAudioPlayback(); // Reset audio playback state for new model
  
  // Force reset playback time
  if (audioCtx) {
    playbackTime = audioCtx.currentTime;
    console.log(`[AUDIO] Force reset playback time to ${playbackTime.toFixed(3)}`);
  }
  
  // Reset audio tracking
  audioChunkCounter = 0;
  lastAudioDataHash = null;
  
  // Start benchmark session
  const selectedModel = modelSelect.value;
  startBenchmarkSession(selectedModel);
  
  logStatus('starting session...');

  openWs();
  await startAudio();

  // send initial presence and sample rate once WS is open (will be retried by server if needed)
  setTimeout(()=> {
    if (ws && ws.readyState === WebSocket.OPEN) {
      const selectedModel = modelSelect.value;
      sendJson({ type: 'hello', sampleRate, ts: Date.now() });
      // Send session start message with selected model to trigger Gemini's initial response
      sendJson({ type: 'sessionStart', model: selectedModel, ts: Date.now() });
    }
  }, 400);
}

function stopSession() {
  // End benchmark session
  endBenchmarkSession();
  
  // send final activityEnd and session_stop
  try {
    if (ws && ws.readyState === WebSocket.OPEN) {
      sendJson({ type: 'activityEnd', ts: Date.now() });
      sendJson({ type: 'session_stop', ts: Date.now() });
      ws.close();
    }
  } catch(e){}

  stopKeepalive();
  if (reconnectTimer) { clearTimeout(reconnectTimer); reconnectTimer = null; }
  stopAudioNodes();
  hideLoader(); // Hide loader when session stops

  startBtn.disabled = false;
  stopBtn.disabled = true;
  logStatus('stopped');
}

// --- Utility: send small diagnostic messages (optional) ---
window.addEventListener('beforeunload', () => {
  try { if (ws && ws.readyState === WebSocket.OPEN) ws.close(); } catch(e){}
});

// --- end of file ---
</script>
</body>
</html>
