<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate" />
  <meta http-equiv="Pragma" content="no-cache" />
  <meta http-equiv="Expires" content="0" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
  <title>AI Voice Assistant</title>
  <style>
    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }
    
    body { 
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      min-height: 100vh;
      padding: 20px;
      color: #333;
    }
    
    .container {
      max-width: 800px;
      margin: 0 auto;
      background: rgba(255, 255, 255, 0.95);
      border-radius: 20px;
      padding: 30px;
      backdrop-filter: blur(10px);
      box-shadow: 0 20px 40px rgba(0, 0, 0, 0.1);
    }
    
    .header {
      text-align: center;
      margin-bottom: 30px;
    }
    
    .header h1 {
      font-size: 2.5rem;
      font-weight: 700;
      background: linear-gradient(135deg, #667eea, #764ba2);
      -webkit-background-clip: text;
      background-clip: text;
      -webkit-text-fill-color: transparent;
      margin-bottom: 10px;
    }
    
    .header p {
      color: #666;
      font-size: 1.1rem;
      font-weight: 300;
    }
    
    .controls {
      display: flex;
      justify-content: center;
      gap: 15px;
      margin-bottom: 25px;
      flex-wrap: wrap;
    }
    
    button { 
      padding: 12px 24px;
      border: none;
      border-radius: 12px;
      font-size: 16px;
      font-weight: 600;
      cursor: pointer;
      transition: all 0.3s ease;
      text-transform: uppercase;
      letter-spacing: 0.5px;
    }
    
    #startBtn {
      background: linear-gradient(135deg, #4CAF50, #45a049);
      color: white;
      box-shadow: 0 4px 15px rgba(76, 175, 80, 0.3);
    }
    
    #startBtn:hover {
      transform: translateY(-2px);
      box-shadow: 0 6px 20px rgba(76, 175, 80, 0.4);
    }
    
    #startBtn:disabled {
      background: #ccc;
      cursor: not-allowed;
      transform: none;
      box-shadow: none;
    }
    
    #stopBtn {
      background: linear-gradient(135deg, #f44336, #d32f2f);
      color: white;
      box-shadow: 0 4px 15px rgba(244, 67, 54, 0.3);
    }
    
    #stopBtn:hover:not(:disabled) {
      transform: translateY(-2px);
      box-shadow: 0 6px 20px rgba(244, 67, 54, 0.4);
    }
    
    #stopBtn:disabled {
      background: #ccc;
      cursor: not-allowed;
      transform: none;
      box-shadow: none;
    }
    
    #testBtn {
      background: linear-gradient(135deg, #2196F3, #1976D2);
      color: white;
      box-shadow: 0 4px 15px rgba(33, 150, 243, 0.3);
    }
    
    #testBtn:hover {
      transform: translateY(-2px);
      box-shadow: 0 6px 20px rgba(33, 150, 243, 0.4);
    }
    
    .audio-meter {
      display: flex;
      align-items: center;
      justify-content: center;
      margin-bottom: 20px;
      gap: 15px;
    }
    
    .meter-label {
      font-weight: 600;
      color: #555;
    }
    
    #meter { 
      width: 200px; 
      height: 8px; 
      background: rgba(0,0,0,0.1); 
      border-radius: 10px; 
      overflow: hidden;
      box-shadow: inset 0 2px 4px rgba(0,0,0,0.1);
    }
    
    #meter > i { 
      display: block; 
      height: 100%; 
      width: 0%; 
      background: linear-gradient(90deg, #4CAF50, #8BC34A);
      border-radius: 10px;
      transition: width 0.1s ease;
    }
    
    #status { 
      text-align: center;
      padding: 10px 20px;
      background: rgba(0,0,0,0.05);
      border-radius: 10px;
      margin-bottom: 20px;
      font-weight: 500;
      text-transform: capitalize;
    }
    
    /* Loader styles */
    #loader {
      text-align: center;
      margin: 20px 0;
      padding: 15px;
      background: linear-gradient(135deg, #e3f2fd, #bbdefb);
      border-radius: 12px;
      border-left: 4px solid #2196F3;
    }
    
    .spinner {
      display: inline-block;
      width: 20px;
      height: 20px;
      border: 3px solid rgba(33, 150, 243, 0.3);
      border-top: 3px solid #2196F3;
      border-radius: 50%;
      animation: spin 1s linear infinite;
      margin-left: 10px;
      vertical-align: middle;
    }
    
    @keyframes spin {
      0% { transform: rotate(0deg); }
      100% { transform: rotate(360deg); }
    }
    
    /* Conversation styles */
    #conversation {
      background: white;
      border-radius: 15px;
      padding: 20px;
      box-shadow: inset 0 2px 10px rgba(0,0,0,0.05);
      min-height: 300px;
      max-height: 500px;
      overflow-y: auto;
    }
    
    .conversation-header {
      text-align: center;
      margin-bottom: 20px;
      padding-bottom: 15px;
      border-bottom: 2px solid #f0f0f0;
    }
    
    .conversation-header h2 {
      font-size: 1.5rem;
      color: #333;
      font-weight: 600;
    }
    
    /* Message styles */
    .message {
      margin-bottom: 15px;
      animation: slideIn 0.3s ease;
    }
    
    @keyframes slideIn {
      from {
        opacity: 0;
        transform: translateY(10px);
      }
      to {
        opacity: 1;
        transform: translateY(0);
      }
    }
    
    .user-message {
      display: flex;
      justify-content: flex-end;
    }
    
    .gemini-message {
      display: flex;
      justify-content: flex-start;
    }
    
    .message-bubble {
      max-width: 70%;
      padding: 12px 18px;
      border-radius: 18px;
      position: relative;
    }
    
    .user-message .message-bubble {
      background: linear-gradient(135deg, #667eea, #764ba2);
      color: white;
      border-bottom-right-radius: 6px;
    }
    
    .gemini-message .message-bubble {
      background: linear-gradient(135deg, #f8f9fa, #e9ecef);
      color: #333;
      border-bottom-left-radius: 6px;
      border: 1px solid #e0e0e0;
    }
    
    .message-label {
      font-size: 11px;
      font-weight: 700;
      text-transform: uppercase;
      letter-spacing: 0.5px;
      margin-bottom: 5px;
      opacity: 0.8;
    }
    
    .user-message .message-label {
      color: rgba(255,255,255,0.9);
    }
    
    .gemini-message .message-label {
      color: #666;
    }
    
    .message-text {
      font-size: 15px;
      line-height: 1.5;
      word-wrap: break-word;
    }
    
    /* Scrollbar styling */
    #conversation::-webkit-scrollbar {
      width: 6px;
    }
    
    #conversation::-webkit-scrollbar-track {
      background: #f1f1f1;
      border-radius: 10px;
    }
    
    #conversation::-webkit-scrollbar-thumb {
      background: linear-gradient(135deg, #667eea, #764ba2);
      border-radius: 10px;
    }
    
    #conversation::-webkit-scrollbar-thumb:hover {
      background: #555;
    }
    
    /* Responsive design */
    @media (max-width: 600px) {
      .container {
        margin: 10px;
        padding: 20px;
      }
      
      .header h1 {
        font-size: 2rem;
      }
      
      .controls {
        flex-direction: column;
        align-items: center;
      }
      
      button {
        width: 100%;
        max-width: 200px;
      }
      
      .message-bubble {
        max-width: 85%;
      }
    }
  </style>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>AI Voice Assistant</h1>
      <p>Start a conversation with your intelligent AI companion</p>
    </div>
    
    <div class="controls">
      <button id="startBtn"><i class="fas fa-play"></i> Start Session</button>
      <button id="stopBtn" disabled><i class="fas fa-stop"></i> Stop Session</button>
      <button id="testBtn"><i class="fas fa-vial"></i> Test Message</button>
    </div>
    
    <div class="audio-meter">
      <span class="meter-label"><i class="fas fa-microphone"></i> Audio Level:</span>
      <div id="meter"><i></i></div>
    </div>
    
    <div id="status">ðŸŸ¢ idle</div>
    
    <div id="loader" style="display: none;">
      <span><i class="fas fa-robot"></i> AI is thinking...</span>
      <div class="spinner"></div>
    </div>
    
    <div id="conversation">
      <div class="conversation-header">
        <h2><i class="fas fa-comments"></i> Conversation</h2>
      </div>
      <div id="messages"></div>
    </div>
  </div>

<script>
/*
  Key design points:
  - Single WS connection reused for whole session.
  - Capture audio with AudioContext + ScriptProcessor and accumulate into ~CHUNK_MS chunks.
  - Send binary Int16 PCM chunks as ArrayBuffer. Precede them with a small JSON control message (depends on server).
  - Adaptive VAD: calibrate initial noise floor then detect voice start/stop using rolling RMS and thresholds.
  - Send `activityStart` when voice begins, `activityEnd` after silence timeout.
  - Play back server-sent audio (binary) by decoding and scheduling with AudioContext.
  - Keepalive pings and simple reconnect/backoff.
  NOTE: adjust SERVER_WS, and confirm server expects: 
        1) control messages as JSON text (e.g., {type: 'activityStart'}), and 
        2) raw PCM Int16 frames as binary. If your server expects Opus/webm blobs instead, switch capture method to MediaRecorder.
*/

const SERVER_WS = "ws://localhost:8000/ws";
const CHUNK_MS = 200;                // send audio approx every X ms
const BUFFER_SIZE = 4096;            // ScriptProcessor buffer size
const SILENCE_DURATION_MS = 1000;    // silence required to send activityEnd
const CALIBRATE_MS = 1200;           // initial ambient noise calibration period
const SILENCE_THRESHOLD_MULT = 3.0;  // voice threshold = noiseFloor * mult
const KEEPALIVE_INTERVAL_MS = 20000; // send ping every 20s

let ws = null;
let audioCtx = null;
let processor = null;
let mediaStream = null;
let sourceNode = null;
let sampleRate = 48000; // will update from audioCtx.sampleRate
let sendBuffer = [];    // accumulate Float32 samples
let sendBufferSize = 0;
let lastSendAt = 0;

// VAD vars
let playbackTime = 0;  // global tracker for sequential audio
let activeAudioSources = [];  // track active audio sources for interruption
let audioSourceCounter = 0;   // assign unique IDs to audio sources for debugging
let noiseFloor = 0;
let calibrated = false;
let calibrationStart = null;
let speaking = false;
let silenceStart = null;
let currentUserMessage = ""; // Track current user speech
let lastUserMessageElement = null; // Track the last user message element for updates
let lastGeminiMessageElement = null; // Track the last Gemini message element for streaming updates
let currentGeminiMessage = ""; // Track current Gemini response being built

// UI
const startBtn = document.getElementById('startBtn');
const stopBtn = document.getElementById('stopBtn');
const testBtn = document.getElementById('testBtn');
const statusEl = document.getElementById('status');
const meterBar = document.querySelector('#meter > i');
const loaderEl = document.getElementById('loader');
const messagesEl = document.getElementById('messages');

console.log('[INIT] DOM elements found:', {
  startBtn: !!startBtn,
  stopBtn: !!stopBtn,
  testBtn: !!testBtn,
  statusEl: !!statusEl,
  meterBar: !!meterBar,
  loaderEl: !!loaderEl,
  messagesEl: !!messagesEl
});

startBtn.onclick = startSession;
stopBtn.onclick = stopSession;
testBtn.onclick = () => {
  console.log('[TEST] Adding test messages');
  addMessage('Test user message', true);
  addMessage('Test Gemini response', false);
};

function logStatus(s) {
  statusEl.textContent = s;
}

function showLoader() {
  loaderEl.style.display = 'block';
}

function hideLoader() {
  loaderEl.style.display = 'none';
}

function addMessage(text, isUser = false, updateLast = false) {
  console.log(`[addMessage] Called with: text="${text}", isUser=${isUser}, updateLast=${updateLast}`);
  console.log(`[addMessage] messagesEl:`, messagesEl);
  
  if (updateLast && isUser && lastUserMessageElement) {
    // Update the existing user message
    const textDiv = lastUserMessageElement.querySelector('.message-bubble .message-text');
    if (textDiv) {
      textDiv.textContent = text;
      console.log(`[addMessage] Updated existing user message to: "${text}"`);
    }
    return;
  }
  
  if (updateLast && !isUser && lastGeminiMessageElement) {
    // Update the existing Gemini message (for streaming)
    const textDiv = lastGeminiMessageElement.querySelector('.message-bubble .message-text');
    if (textDiv) {
      textDiv.textContent = text;
      console.log(`[addMessage] Updated existing Gemini message to: "${text}"`);
      // Auto-scroll to bottom
      messagesEl.scrollTop = messagesEl.scrollHeight;
    }
    return;
  }
  
  const messageDiv = document.createElement('div');
  messageDiv.className = `message ${isUser ? 'user-message' : 'gemini-message'}`;
  
  const bubbleDiv = document.createElement('div');
  bubbleDiv.className = 'message-bubble';
  
  const labelDiv = document.createElement('div');
  labelDiv.className = 'message-label';
  labelDiv.textContent = isUser ? 'ðŸ‘¤ You' : 'ðŸ¤– AI Assistant';
  
  const textDiv = document.createElement('div');
  textDiv.className = 'message-text';
  textDiv.textContent = text;
  
  bubbleDiv.appendChild(labelDiv);
  bubbleDiv.appendChild(textDiv);
  messageDiv.appendChild(bubbleDiv);
  
  console.log('[addMessage] Created messageDiv:', messageDiv);
  console.log('[addMessage] messagesEl before appendChild:', messagesEl);
  
  messagesEl.appendChild(messageDiv);
  
  console.log('[addMessage] messagesEl after appendChild, children count:', messagesEl.children.length);
  console.log(`[addMessage] Added new message: "${text}" (${isUser ? 'user' : 'gemini'})`);
  console.log(`[addMessage] Total messages now:`, messagesEl.children.length);
  
  if (isUser) {
    lastUserMessageElement = messageDiv;
    console.log(`[addMessage] Set lastUserMessageElement`);
  } else {
    lastGeminiMessageElement = messageDiv;
    console.log(`[addMessage] Set lastGeminiMessageElement`);
  }
  
  // Auto-scroll to bottom
  messagesEl.scrollTop = messagesEl.scrollHeight;
}

function clearMessages() {
  messagesEl.innerHTML = '';
  lastUserMessageElement = null;
  lastGeminiMessageElement = null;
  currentGeminiMessage = "";
}

// Helper function to track audio sources
function trackAudioSource(src, type) {
  const id = ++audioSourceCounter;
  src._debugId = id;
  activeAudioSources.push(src);
  console.log(`[AUDIO] Started ${type} audio (ID: ${id}, ${activeAudioSources.length} total sources)`);
  
  src.onended = () => {
    const index = activeAudioSources.indexOf(src);
    if (index > -1) {
      activeAudioSources.splice(index, 1);
      console.log(`[AUDIO] ${type} audio ended naturally (ID: ${id}, ${activeAudioSources.length} remaining)`);
    }
  };
  
  return src;
}

// --- WebSocket helpers ---
let pingTimer = null;
let reconnectTimer = null;
let reconnectAttempts = 0;

function openWs() {
  if (ws && ws.readyState === WebSocket.OPEN) return;
  ws = new WebSocket(SERVER_WS);
  ws.binaryType = 'arraybuffer';

  ws.onopen = () => {
    console.log('WS open');
    reconnectAttempts = 0;
    logStatus('connected');
    startKeepalive();
    // optionally send session-start control
    sendJson({ type: 'session_start', sampleRate });
  };

  ws.onmessage = async (ev) => {
    console.log('[CLIENT] Raw message received:', ev.data);
    console.log('[CLIENT] Message type:', typeof ev.data);
    
    // If server sends text JSON
    if (typeof ev.data === 'string') {
      try {
        const msg = JSON.parse(ev.data);
        console.log('[CLIENT] Parsed JSON message:', msg);
        console.log('[CLIENT] Message type field:', msg.type);
        console.log('[CLIENT] Message text field:', msg.text);
        
        // Handle interruption signal from server
        if (msg.type === 'interrupt' && msg.message === 'stop_playback') {
          console.log('[CLIENT] Handling interrupt signal');
          stopAllAudioPlayback();
          hideLoader(); // Hide loader if interruption occurs
          return;
        }
        
        // Handle text responses from Gemini
        if (msg.type === 'text' && msg.text) {
          console.log('[CLIENT] Handling text response from Gemini:', msg.text);
          console.log('[CLIENT] messagesEl exists:', !!messagesEl);
          console.log('[CLIENT] lastGeminiMessageElement exists:', !!lastGeminiMessageElement);
          
          // Append this chunk to the current Gemini message
          currentGeminiMessage += msg.text;
          console.log('[CLIENT] Current Gemini message:', currentGeminiMessage);
          
          if (!lastGeminiMessageElement) {
            // Create new message if none exists
            console.log('[CLIENT] Creating new Gemini message');
            addMessage(currentGeminiMessage, false);
            console.log('[CLIENT] After addMessage - lastGeminiMessageElement:', !!lastGeminiMessageElement);
          } else {
            // Update existing message with accumulated text
            console.log('[CLIENT] Updating existing Gemini message');
            addMessage(currentGeminiMessage, false, true);
          }
          
          console.log('[CLIENT] Called addMessage for Gemini response');
          return;
        }
        
        // Handle transcriptions from server
        if (msg.type === 'transcript') {
          console.log(`[CLIENT] Handling ${msg.source} transcript:`, msg.text);
          if (msg.source === 'input') {
            // User speech transcription - update in real-time
            currentUserMessage = msg.text;
            if (speaking) {
              // If user is still speaking, show live transcription
              if (!lastUserMessageElement) {
                console.log('[CLIENT] Adding new user transcript message');
                addMessage(msg.text, true);
              } else {
                console.log('[CLIENT] Updating existing user transcript message');
                addMessage(msg.text, true, true); // Update existing message
              }
            }
          } else if (msg.source === 'output') {
            // Gemini speech transcription - display this as the text response
            console.log('[CLIENT] Gemini speech transcript:', msg.text);
            
            // Append this chunk to the current Gemini message
            currentGeminiMessage += msg.text;
            console.log('[CLIENT] Current Gemini message from transcript:', currentGeminiMessage);
            
            if (!lastGeminiMessageElement) {
              // Create new message if none exists
              console.log('[CLIENT] Creating new Gemini message from transcript');
              addMessage(currentGeminiMessage, false);
              console.log('[CLIENT] After addMessage - lastGeminiMessageElement:', !!lastGeminiMessageElement);
            } else {
              // Update existing message with accumulated text
              console.log('[CLIENT] Updating existing Gemini message from transcript');
              addMessage(currentGeminiMessage, false, true);
            }
          }
          return;
        }
        
        // handle other control text messages as needed
        console.log('[CLIENT] Unhandled JSON message type:', msg.type, msg);
        return;
      } catch (e) {
        console.log('[CLIENT] Failed to parse JSON:', e);
        console.log('[CLIENT] Non-JSON text received:', ev.data);
        return;
      }
    }

    // binary: assume audio bytes (raw PCM16 24kHz from Gemini) â€” decode & play
    if (ev.data instanceof ArrayBuffer) {
      console.log(`[AUDIO] Received audio data, size: ${ev.data.byteLength} bytes`);
      hideLoader(); // Hide loader when audio starts arriving
      try {
        // Try decodeAudioData â€” this assumes the server sent a valid encoded audio container (e.g. WAV/Opus)
        const audioData = ev.data;
        audioCtx.decodeAudioData(audioData.slice(0)).then(decoded => {
          console.log('[AUDIO] Successfully decoded audio, playing via decodeAudioData');
          playDecodedAudio(decoded);
        }).catch(err => {
          // Fallback: treat as raw PCM16LE 24000Hz mono
          console.log('[AUDIO] decodeAudioData failed â€” playing as raw PCM16 24kHz');
          playRawPCM16(audioData, 24000);
        });
      } catch (err) {
        console.error('ws binary handling error', err);
      }
    }
  };

  ws.onclose = (e) => {
    console.log('WS closed', e);
    stopKeepalive();
    hideLoader(); // Hide loader if connection is lost
    logStatus('disconnected');
    scheduleReconnect();
  };

  ws.onerror = (e) => {
    console.error('WS error', e);
    hideLoader(); // Hide loader on error
  };
}

// Play raw PCM16LE mono at given sample rate, scheduled sequentially
function playRawPCM16(buffer, sampleRate = 24000) {
  if (!audioCtx) audioCtx = new (window.AudioContext || window.webkitAudioContext)();
  const int16 = new Int16Array(buffer);
  const float32 = new Float32Array(int16.length);
  for (let i = 0; i < int16.length; i++) {
    float32[i] = int16[i] / 32768;
  }
  const audioBuffer = audioCtx.createBuffer(1, float32.length, sampleRate);
  audioBuffer.getChannelData(0).set(float32);
  const src = audioCtx.createBufferSource();
  src.buffer = audioBuffer;
  src.connect(audioCtx.destination);
  
  // Track this audio source for potential interruption
  trackAudioSource(src, 'raw-PCM16');
  
  // Schedule playback sequentially
  const now = audioCtx.currentTime;
  if (playbackTime < now) {
    playbackTime = now;
  }
  src.start(playbackTime);
  playbackTime += audioBuffer.duration;
}

function scheduleReconnect() {
  if (reconnectTimer) return;
  reconnectAttempts++;
  const backoff = Math.min(30000, 1000 * Math.pow(1.5, reconnectAttempts));
  console.log('reconnect in', backoff);
  reconnectTimer = setTimeout(() => {
    reconnectTimer = null;
    openWs();
  }, backoff);
}

function startKeepalive(){
  stopKeepalive();
  pingTimer = setInterval(() => {
    if (ws && ws.readyState === WebSocket.OPEN) {
      try { ws.send(JSON.stringify({type:'ping', ts: Date.now()})); }
      catch(e){}
    }
  }, KEEPALIVE_INTERVAL_MS);
}
function stopKeepalive(){ if (pingTimer) { clearInterval(pingTimer); pingTimer = null; } }
function sendJson(obj){ if (ws && ws.readyState === WebSocket.OPEN) ws.send(JSON.stringify(obj)); }

// --- Audio capture, chunking, encoding to Int16 ---
function startAudio() {
  audioCtx = new (window.AudioContext || window.webkitAudioContext)();
  sampleRate = audioCtx.sampleRate;
  return navigator.mediaDevices.getUserMedia({audio:true})
    .then(stream => {
      mediaStream = stream;
      sourceNode = audioCtx.createMediaStreamSource(stream);

      // ScriptProcessorNode for capture (simple, widely supported)
      processor = audioCtx.createScriptProcessor(BUFFER_SIZE, 1, 1);
      sourceNode.connect(processor);
      processor.connect(audioCtx.destination); // required in some browsers

      // reset send buffer
      sendBuffer = [];
      sendBufferSize = 0;
      calibrated = false;
      noiseFloor = 0;
      calibrationStart = performance.now();

      processor.onaudioprocess = onAudioProcess;
      logStatus('listening for voice...');
    });
}

function stopAudioNodes() {
  if (processor) { processor.disconnect(); processor.onaudioprocess = null; processor = null; }
  if (sourceNode) { try { sourceNode.disconnect(); } catch(e){} sourceNode = null; }
  if (mediaStream) { mediaStream.getTracks().forEach(t => t.stop()); mediaStream = null; }
  if (audioCtx) { try { audioCtx.close(); } catch(e){} audioCtx = null; }
}

// accumulate frames, send when we cross CHUNK_MS window
function onAudioProcess(ev) {
  const input = ev.inputBuffer.getChannelData(0);
  const now = performance.now();

  // compute RMS and update UI meter
  let sum = 0;
  for (let i=0;i<input.length;i++){ sum += input[i]*input[i]; }
  const rms = Math.sqrt(sum / input.length);
  updateMeter(rms);

  // calibration
  if (!calibrated) {
    if (now - calibrationStart < CALIBRATE_MS) {
      noiseFloor = noiseFloor === 0 ? rms : (noiseFloor * 0.95 + rms * 0.05);
    } else {
      calibrated = true;
      noiseFloor = Math.max(1e-6, noiseFloor); // avoid zero
      console.log('calibrated noiseFloor', noiseFloor);
    }
  }

  // VAD logic
  const voiceThreshold = Math.max(1e-6, noiseFloor * SILENCE_THRESHOLD_MULT);
  if (calibrated && rms > voiceThreshold) {
    // voice present
    silenceStart = null;
    if (!speaking) {
      speaking = true;
      
      // Reset Gemini message tracking for new turn
      currentGeminiMessage = "";
      lastGeminiMessageElement = null;
      
      // Don't immediately stop audio here - let the server decide if interruption is needed
      // The server will send an interrupt signal only if Gemini is actually speaking
      console.log('User started speaking - sending activityStart to server');
      
      // Create initial user message placeholder
      addMessage('...', true);
      
      sendJson({ type: 'activityStart', ts: Date.now() });
      console.log('>>> activityStart');
      logStatus('speaking');
    }
  } else {
    // low volume
    if (speaking) {
      if (!silenceStart) silenceStart = now;
      else if (now - silenceStart >= SILENCE_DURATION_MS) {
        // end of user turn
        speaking = false;
        silenceStart = null;
        sendJson({ type: 'activityEnd', ts: Date.now() });
        console.log('>>> activityEnd');
        
        // Finalize user message with transcription or fallback
        if (currentUserMessage && currentUserMessage !== "...") {
          // We have a real transcription, make sure it's displayed
          if (lastUserMessageElement) {
            addMessage(currentUserMessage, true, true);
          } else {
            addMessage(currentUserMessage, true);
          }
        } else if (lastUserMessageElement) {
          // No transcription received, show fallback
          addMessage('[Voice message]', true, true);
        } else {
          // Fallback if no message element exists
          addMessage('[Voice message]', true);
        }
        
        currentUserMessage = "";
        
        logStatus('idle (waiting for response...)');
        
        // Reset Gemini message tracking for new response
        currentGeminiMessage = "";
        lastGeminiMessageElement = null;
        
        showLoader(); // Show loader when waiting for Gemini's response
      }
    } else {
      // not speaking, optionally adapt noise floor slowly
      if (calibrated) {
        // adapt noise floor slowly so environment drift is handled
        noiseFloor = noiseFloor * 0.999 + rms * 0.001;
      }
    }
  }

  // Only send audio chunks if currently speaking (between activityStart and activityEnd)
  if (speaking) {
    // copy into accumulator
    // keep a typed copy to avoid referencing float32 view later
    sendBuffer.push(new Float32Array(input));
    sendBufferSize += input.length;

    // check if enough samples to send chunk (CHUNK_MS)
    const samplesPerMs = sampleRate / 1000;
    const targetSamples = Math.floor(samplesPerMs * CHUNK_MS);

    if (sendBufferSize >= targetSamples) {
      // merge into single Float32Array
      const out = new Float32Array(sendBufferSize);
      let offset = 0;
      for (let buf of sendBuffer) {
        out.set(buf, offset);
        offset += buf.length;
      }
      // reset accumulator
      sendBuffer = [];
      sendBufferSize = 0;

      // convert to int16
      const int16 = floatTo16BitPCM(out);

      // send a small header JSON before the binary so server can interpret if needed
      if (ws && ws.readyState === WebSocket.OPEN) {
        // header includes type and sampleRate so server can decode appropriately
        // try { ws.send(JSON.stringify({ type:'input_audio_chunk', encoding:'pcm_s16', sampleRate: sampleRate, ts: Date.now() })); } catch(e){}
        try { ws.send(int16.buffer); } catch(e){ console.error('send binary fail', e); }
        lastSendAt = Date.now();
      }
    }
  }
}

// float32 -> Int16 (little endian)
function floatTo16BitPCM(float32Array) {
  const l = float32Array.length;
  const buffer = new ArrayBuffer(l * 2);
  const view = new DataView(buffer);
  let offset = 0;
  for (let i = 0; i < l; i++, offset += 2) {
    let s = Math.max(-1, Math.min(1, float32Array[i]));
    view.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7FFF, true);
  }
  return new Int16Array(buffer);
}

// --- Playback of decoded AudioBuffer ---
function stopAllAudioPlayback() {
  // Stop all currently playing audio sources
  console.log(`[AUDIO] Attempting to stop ${activeAudioSources.length} active audio sources`);
  
  let stoppedCount = 0;
  const sourcesToStop = [...activeAudioSources]; // Create copy to avoid array modification issues
  
  for (let i = 0; i < sourcesToStop.length; i++) {
    const source = sourcesToStop[i];
    try {
      // Check if source is still playable
      if (source && typeof source.stop === 'function') {
        source.stop(0); // Stop immediately
        stoppedCount++;
        console.log(`[AUDIO] Stopped audio source ID: ${source._debugId || 'unknown'}`);
      }
    } catch (e) {
      // Source might already be stopped or finished
      console.log(`[AUDIO] Source ID ${source._debugId || 'unknown'} already stopped:`, e.message);
    }
  }
  
  // Clear the entire array
  activeAudioSources.length = 0;
  
  console.log(`[AUDIO] Successfully stopped ${stoppedCount} audio sources, cleared tracking array`);
  
  // Reset the sequential playback time to current time
  if (audioCtx) {
    const oldPlaybackTime = playbackTime;
    playbackTime = audioCtx.currentTime;
    console.log(`[AUDIO] Reset playback time from ${oldPlaybackTime.toFixed(3)} to ${playbackTime.toFixed(3)}`);
  }
}

function playDecodedAudio(audioBuffer) {
  if (!audioCtx) audioCtx = new (window.AudioContext || window.webkitAudioContext)();
  const src = audioCtx.createBufferSource();
  src.buffer = audioBuffer;
  src.connect(audioCtx.destination);
  
  // Track this audio source for potential interruption
  trackAudioSource(src, 'decoded');
  
  src.start();
}

// Play raw PCM16LE mono at given sample rate, scheduled sequentially
function playRawPCM16(buffer, sampleRate = 24000) {
  if (!audioCtx) audioCtx = new (window.AudioContext || window.webkitAudioContext)();
  const int16 = new Int16Array(buffer);
  const float32 = new Float32Array(int16.length);
  for (let i = 0; i < int16.length; i++) {
    float32[i] = int16[i] / 32768;
  }
  const audioBuffer = audioCtx.createBuffer(1, float32.length, sampleRate);
  audioBuffer.getChannelData(0).set(float32);
  const src = audioCtx.createBufferSource();
  src.buffer = audioBuffer;
  src.connect(audioCtx.destination);
  
  // Track this audio source for potential interruption
  trackAudioSource(src, 'raw-PCM16');
  
  // Schedule playback sequentially
  const now = audioCtx.currentTime;
  if (playbackTime < now) {
    playbackTime = now;
  }
  src.start(playbackTime);
  playbackTime += audioBuffer.duration;
}
// --- UI helpers ---
function updateMeter(rms) {
  // convert rms to 0..1 scale (rough)
  // scale by 10x so quiet voices are visible; tune as required
  const v = Math.min(1, Math.sqrt(rms) * 10);
  meterBar.style.width = `${(v*100).toFixed(1)}%`;
}

// --- Control flow: start/stop session ---
async function startSession() {
  startBtn.disabled = true;
  stopBtn.disabled = false;
  hideLoader(); // Make sure loader starts hidden
  clearMessages(); // Clear previous conversation
  logStatus('starting session...');

  openWs();
  await startAudio();

  // send initial presence and sample rate once WS is open (will be retried by server if needed)
  setTimeout(()=> {
    if (ws && ws.readyState === WebSocket.OPEN) {
      sendJson({ type: 'hello', sampleRate, ts: Date.now() });
      // Send session start message to trigger Gemini's initial response
      sendJson({ type: 'sessionStart', ts: Date.now() });
    }
  }, 400);
}

function stopSession() {
  // send final activityEnd and session_stop
  try {
    if (ws && ws.readyState === WebSocket.OPEN) {
      sendJson({ type: 'activityEnd', ts: Date.now() });
      sendJson({ type: 'session_stop', ts: Date.now() });
      ws.close();
    }
  } catch(e){}

  stopKeepalive();
  if (reconnectTimer) { clearTimeout(reconnectTimer); reconnectTimer = null; }
  stopAudioNodes();
  hideLoader(); // Hide loader when session stops

  startBtn.disabled = false;
  stopBtn.disabled = true;
  logStatus('stopped');
}

// --- Utility: send small diagnostic messages (optional) ---
window.addEventListener('beforeunload', () => {
  try { if (ws && ws.readyState === WebSocket.OPEN) ws.close(); } catch(e){}
});

// --- end of file ---
</script>
</body>
</html>
